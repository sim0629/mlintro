\documentclass[a4paper,10pt]{article}

\usepackage{kotex}
\usepackage{datetime}
\usepackage{fullpage}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{listings}

\graphicspath{ {images/} }

\newdateformat{koreandate}{\THEYEAR년 \twodigit{\THEMONTH}월 \twodigit{\THEDAY}일}

\renewcommand{\abstractname}{초록}
\renewcommand{\figurename}{그림}
\renewcommand{\tablename}{표}
\renewcommand{\contentsname}{목차}
%\renewcommand{\listfigurename}{그림 목차}
%\renewcommand{\listtablename}{표 목차}

\linespread{1.5}

\begin{document}
\title{선행 연구(Coates et al., 2011)에 기초한 \\
CIFAR-10 이미지 분류기 구현 및 실험}
\author{기계학습개론 기말 프로젝트 보고서 \\
컴퓨터공학부 2009-11744 심규민}
\date{\koreandate\today}
\maketitle

\pagenumbering{arabic}

\begin{abstract}
(TODO)
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\section{서론}

Coates et al.(2011)의 연구에서는 무감독 학습을 통해 얻은 feature로 단일 층(SVM)으로 구성된 이미지 분류기를 만들고, 무감독 학습 알고리즘과 하이퍼 파라미터를 바꿔보며 성능 면에서 비교를 하였다.
본 실험에서는 이 선행 연구에서 구현한 분류기에 기초하여, 하이퍼 파라미터는 선행 연구에서 찾아낸 가장 좋은 성능을 내는 것으로 고정하고, feature extraction에 사용한 무감독 학습 알고리즘(특히, K-means 알고리즘)과 pooling 방식을 다양하게 추가 구현하여 선행 연구와 비교해 보았다.

먼저, 선행 연구에서는 feature 학습에 다른 복잡한 알고리즘들에 비해 상대적으로 간단한 K-means 알고리즘을 사용했을 때 가장 좋은 성능을 냈다고 강조하였다.
그러나 실은 가장 기본적인 K-means(hard) 알고리즘을 사용한 것이 아니라 triangle activation을 더한 K-means(triangle) 알고리즘을 사용하였다.
성능(분류 정확도)에 있어서도 K-means(hard) 알고리즘을 사용했을 때에는 다른 알고리즘들을 사용했을 때 보다 낮은 성능을 보였으며, K-means(triangle) 알고리즘을 사용했을 때 비로소 가장 좋은 성능을 내었다.
이 사실에 주목하여 soft한 특성을 갖도록 다양하게 변형한 K-means 알고리즘들에 대해서도 실험 해보기로 하였다.
또한, 선행 연구에서는 convolutional feature extraction에 사용한 receptive field 크기, stride, feature 개수, whitening 여부 등의 하이퍼 파라미터들은 다양하게 바꿔보며 실험한 데에 반해서, pooling 방식은 summation만을 사용하였다.
이 사실에 의문을 갖고 average pooling, maximum pooling, stochastic pooling(Zeiler et al., 2013), stochastic max-pooling(Huang et al., 2015) 등의 다양한 pooling 방식에 대해서도 실험 해보기로 하였다.
마지막으로, 선행 연구에서는 분류 정확도를에 대한 오차 범위가 드러나 있지 않다.
다양한 soft K-means 알고리즘과 pooling 방식에 대한 비교를 더욱 정밀히 하기 위해, 여러번 실험하여 오차 범위까지 산출하여 분석 해보기로 하였다.

앞으로 본 보고서에서는 구현의 명확성을 위해 선행 연구의 방법을 더 자세히 설명하고, 본 실험을 위해 추가로 구현한 사항을 명시한 뒤, 실험 결과를 분석하였다.

\section{선행 연구}

선행 연구(Coates et al., 2011)에서 가장 좋은 성능을 낸 이미지 분류기의 구체적인 알고리즘은 다음과 같다.
\begin{enumerate}
\item 랜덤 patch 샘플링
\item K-means 알고리즘을 통한 feature extractor 생성
\item Convolutional feature 추출 (training 데이터)
\item Summation over quadrant pooling (training 데이터)
\item SVM classifier 학습
\item Convolutional feature 추출 (testing 데이터)
\item Summation over quadrant pooling (testing 데이터)
\item 학습한 SVM으로 분류
\end{enumerate}
전체는 크게 학습 단계(1-5)와 분류 단계(6-8)로 나뉜다.
이 중 convolutional feature 추출과, pooling은 training 데이터와 testing 데이터 모두에게 공통적으로 적용된다.
각 세부 단계의 구체적인 구현은 다음과 같다.

\subsection{랜덤 patch 샘플링}

CIFAR-10의 training 데이터는 50000개의 32x32픽셀 컬러(RGB의 3채널) 이미지와 분류 레이블로 구성되어 있다.
즉,
\begin{align*}
    X_{train} &= \{ \mathbf{x}^{(i)} \}_{i=1}^{50000} ( \forall i, \mathbf{x}^{(i)} \in [0, 1]^{32 \times 32 \times 3} ) \\
    Y_{train} &= \{ y^{(i)} \}_{i=1}^{50000} ( \forall i, y^{(i)} \in \{0, 1, ..., 9\} )
\end{align*}
$X_{train}$에서 무작위로 6x6x3 크기의 patch를 400000개 뽑는다.
이 때 공평하게 각각의 $\mathbf{x}^{(i)}$에서 8개씩 뽑는다.
이렇게 하여,
\begin{align*}
    P_{sample} &= \{ \mathbf{p}^{(i)} \}_{i=1}^{400000} ( \forall i, \mathbf{p}^{(i)} \in [0, 1]^{6 \times 6 \times 3} )
\end{align*}
를 얻는다.

\subsection{Feature extractor 생성}

\subsection{Convolutional feature 추출}

\subsection{Summation over quadrant pooling}

\subsection{SVM classification}


\section{추가 구현}

\section{실험 결과}

\section{결론}

triangle k-means는 activation에 ReLU를 적용한 것과 사실상 같음

\section*{참고 문헌}

\begin{itemize}
\item Coates, A., Ng, A. Y., \& Lee, H. (2011). An analysis of single-layer networks in unsupervised feature learning. In \textit{International conference on artificial intelligence and statistics}.
\item Huang, Y., Sun, X., Lu, M., \& Xu, M. (2015). Channel-Max, Channel-Drop and Stochastic Max-Pooling. In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops}.
\item Zeiler, M. D., \& Fergus, R. (2013). Stochastic pooling for regularization of deep convolutional neural networks. \textit{arXiv preprint arXiv:1301.3557}.
\end{itemize}

\end{document}
